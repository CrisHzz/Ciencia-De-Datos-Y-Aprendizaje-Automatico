{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regresion logistica usando tensorflow\n",
    "\n",
    "Luego de aprender lo basico y de que trata la regresion logistica en scikit learn, podemos adentrarnos mas y hacer un modelo por nuestra cuenta usando tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Cristian\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Usaremos nuevamente el dataset de iris que en pocas palabras se trata de colores de flores con sus petalos,sepalos,tamaños y dimensiones**\n",
    "\n",
    "1. El sépalo es una parte de la flor que protege y sostiene los órganos reproductores \n",
    "\n",
    "2. El pétalo es la parte de la flor que a menudo es colorida y atractiva para los polinizadores\n",
    "\n",
    "Variables son \n",
    "\n",
    "**Attributes Independent Variable**\n",
    "\n",
    "- petal length (longitud petalo)\n",
    "- petal width  (ancho del petalo)\n",
    "- sepal length (Longitud del sepalo)\n",
    "- sepal width (ancho del sépalo)\n",
    "\n",
    "**Dependent Variable**\n",
    "- Species\n",
    "- Iris setosa (tipo de planta)\n",
    "- Iris virginica\n",
    "- Iris versicolor\n",
    "\n",
    "![partesFlor](220px-Pétalo-sépalo.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris=load_iris()\n",
    "\n",
    "iris_x,iris_y=iris.data[:-1,:], iris.target[:-1]#Verificar que tendran el mismo tamaño de datos iris_X y iris_y\n",
    "\n",
    "iris_y=pd.get_dummies(iris_y).values #Se crea un dummie con el tipo de iris de las flores para categorizar mejor, luego values lo pasa a un arrelglo para el modelo luego\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(149, 4)\n",
      "(149, 3)\n"
     ]
    }
   ],
   "source": [
    "print(iris_x.shape)\n",
    "\n",
    "print(iris_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Como vemos si son compatibles**\n",
    "\n",
    "- iris.data[:-1,-1:]: Esto selecciona todas las filas de iris.data excepto la última fila ([:-1]), y solo la última columna ([-1:]). En otras palabras, está seleccionando la última característica de todas las flores de iris excepto la última.\n",
    "  \n",
    "- iris.target[:-1]: Esto selecciona todas las filas de iris.target excepto la última fila ([:-1]). En otras palabras, está seleccionando las etiquetas de todas las flores de iris excepto la última."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numFeatures is :  4\n"
     ]
    }
   ],
   "source": [
    "# train ,test siempre en ese orden \n",
    "x_train,x_test,y_train,y_Test =train_test_split(iris_x,iris_y,test_size=0.37,random_state=47)\n",
    "\n",
    "numFeatures=x_train.shape[1] #Se le da un numero de etiquetas a los datos x de entrenamiento son las 4 tipos de variables que existen\n",
    "\n",
    "# [0] para sus filas y [1] para sus columnas\n",
    "\n",
    "print('numFeatures is : ', numFeatures ) #Columnas de x_train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numLabels is :  3\n"
     ]
    }
   ],
   "source": [
    "numLabels = y_train.shape[1]\n",
    "print('numLabels is : ', numLabels ) #Columnas de  y_Train\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creamos nuestros tensores Constantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train=tf.constant(x_train,dtype='float32')\n",
    "\n",
    "x_test=tf.constant(x_test,dtype='float32')\n",
    "\n",
    "y_train=tf.constant(y_train,dtype='float32')\n",
    "\n",
    "y_Test=tf.constant(y_Test,dtype='float32')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Recordemos varias formulas que utilizaremos para la regresion logistica\n",
    "\n",
    "$$\\text{Regresion lineal}$$\n",
    "\n",
    "$$y=MX+B$$\n",
    "\n",
    "$$\\text{Funcion sigmoide }$$\n",
    "# Funcion sigmoide\n",
    "\n",
    "La función sigmoide es una función matemática que tiene una curva característica en forma de “S”. Esta función transforma los valores entre el rango 0 y 1.\n",
    "\n",
    "La expresión matemática para la función sigmoide es\n",
    "\n",
    "$$\\sigma=\\frac{1}{1+e^{-MX}}$$\n",
    "\n",
    "La función sigmoide es diferenciable y tiene una derivada no negativa en cada punto. Además, tiene exactamente un punto de inflexión1.\n",
    "\n",
    "Esta función es especialmente útil en varias aplicaciones, incluyendo las redes neuronales artificiales, donde se utiliza como una función de activación. En este contexto, la función sigmoide se utiliza para mapear los valores de entrada de las neuronas a un rango entre 0 y 1, lo que puede interpretarse como una probabilidad.\n",
    "\n",
    "\n",
    "![funcionSigmoide](sigmoideFunction.png)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <h3>Establecer pesos y sesgos del modelo</h3>\n",
    "\n",
    "Al igual que la regresión lineal, necesitamos una matriz de ponderación de variable compartida para la regresión logística. Inicializamos tanto <code>M</code> como <code>b</code> como tensores llenos de ceros. Como vamos a aprender <code>M</code> y <code>b</code>, su valor inicial no importa demasiado. Estas variables son los objetos que definen la estructura de nuestro modelo de regresión y podemos guardarlas después de haberlas entrenado para poder reutilizarlas más tarde.\n",
    "\n",
    "Definimos dos variables de TensorFlow como nuestros parámetros. Estas variables contendrán los pesos y sesgos de nuestra regresión logística y se actualizarán continuamente durante el entrenamiento.\n",
    "\n",
    "Observe que <code>M</code> tiene una forma de [4, 3] porque queremos multiplicar los vectores de entrada de 4 dimensiones para producir vectores de evidencia de 3 dimensiones para las clases de diferencia. <code>b</code> tiene la forma de [3], por lo que podemos agregarlo a la salida. Las variables de TensorFlow deben inicializarse con valores, p. con ceros.\n",
    "\n",
    "Este 4 y 3 salen de las columnas obtenidas de `x_Train`, `y_train`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = tf.Variable(tf.zeros([4, 3]))  # Su tamaño sale de las dimensiones entre los datos de x y y de entreno de las columnas\n",
    "\n",
    "b = tf.Variable(tf.zeros([3])) # 3-dimensional tensor , es de 3 columnas pues son 3 las clases a clasificar segun el dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = tf.Variable(tf.random.normal([numFeatures,numLabels], #Pesos del modelo\n",
    "                                       mean=0.,\n",
    "                                       stddev=0.01,\n",
    "                                       name=\"weights\"),dtype='float32')\n",
    "\n",
    "\n",
    "bias = tf.Variable(tf.random.normal([1,numLabels], #Sesgo del modelo\n",
    "                                    mean=0.,\n",
    "                                    stddev=0.01,\n",
    "                                    name=\"bias\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este fragmento de código está inicializando los pesos y el sesgo para un modelo de regresión logística utilizando TensorFlow. Aquí te explico cada parte:\n",
    "\n",
    "- `weights = tf.Variable(tf.random.normal([numFeatures,numLabels], mean=0., stddev=0.01, name=\"weights\"),dtype='float32')`: Aquí se está creando una variable TensorFlow para los pesos del modelo. Se inicializan con valores aleatorios extraídos de una distribución normal con media 0 y desviación estándar 0.01. La forma de la matriz de pesos es `[numFeatures,numLabels]`, lo que significa que hay un peso para cada combinación de característica y etiqueta.\n",
    "\n",
    "- `bias = tf.Variable(tf.random.normal([1,numLabels], mean=0., stddev=0.01, name=\"bias\"))`: Aquí se está creando una variable TensorFlow para el sesgo del modelo. Al igual que los pesos, el sesgo se inicializa con valores aleatorios extraídos de una distribución normal con media 0 y desviación estándar 0.01. La forma de la matriz de sesgo es `[1,numLabels]`, lo que significa que hay un término de sesgo para cada etiqueta.\n",
    "\n",
    "`tf.random.normal` es una función en TensorFlow que genera una matriz de números aleatorios, cada uno de los cuales sigue una distribución normal (también conocida como distribución gaussiana).\n",
    "\n",
    "Los parámetros de la función son:\n",
    "\n",
    "- `shape`: Esta es la forma de la matriz de salida que quieres generar. Por ejemplo, si quieres una matriz de 2x3, pasarías `[2, 3]` como la forma.\n",
    "\n",
    "- `mean`: Este es el promedio de la distribución normal que quieres usar. Por defecto es 0.\n",
    "\n",
    "- `stddev`: Esta es la desviación estándar de la distribución normal que quieres usar. Por defecto es 1.\n",
    "\n",
    "- `dtype`: Este es el tipo de datos de los números en la matriz de salida. Por defecto es `tf.float32`.\n",
    "\n",
    "- `seed`: Este es el valor inicial para la generación de números aleatorios. Si especificas una semilla, podrás reproducir los mismos números aleatorios cada vez que ejecutes tu código.\n",
    "\n",
    "- `name`: Este es un nombre para la operación. Esto puede ser útil si estás construyendo un gráfico de TensorFlow más grande y quieres dar nombres a las operaciones específicas.\n",
    "\n",
    "Estos pesos y sesgos se actualizarán durante el entrenamiento del modelo para minimizar la función de pérdida y mejorar la precisión del modelo.\n",
    "\n",
    "\n",
    "## $$W=M$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Modelo de regresión logística</h3>\n",
    "\n",
    "Ahora definimos nuestras operaciones para ejecutar correctamente la Regresión Logística. Generalmente se piensa en la regresión logística como una ecuación única:\n",
    "\n",
    "$$\n",
    "ŷ =sigmoid(WX+b)\n",
    "$$\n",
    "\n",
    "Sin embargo, en aras de la claridad, podemos dividirlo en sus tres componentes principales:\n",
    "\n",
    "- un peso multiplicado presenta una operación de multiplicación de matrices,\n",
    "- una suma de las características ponderadas y un término de sesgo,\n",
    "- y finalmente la aplicación de una función sigmoidea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression(x):\n",
    "    apply_weights_OP = tf.matmul(x, weights, name=\"apply_weights\")\n",
    "    add_bias_OP = tf.add(apply_weights_OP, bias, name=\"add_bias\") \n",
    "    activation_OP = tf.nn.sigmoid(add_bias_OP, name=\"activation\")\n",
    "    return activation_OP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta función `logistic_regression` implementa la ecuación de la regresión logística. \n",
    "\n",
    "1. `apply_weights_OP = tf.matmul(x, weights, name=\"apply_weights\")`: Esta línea realiza la multiplicación matricial de los datos de entrada `x` con los pesos del modelo. Esto es equivalente a aplicar los pesos a las características de entrada.\n",
    "\n",
    "2. `add_bias_OP = tf.add(apply_weights_OP, bias, name=\"add_bias\")`: Esta línea suma el sesgo al resultado de la multiplicación matricial de la línea anterior. Esto es equivalente a añadir el término de sesgo a la ecuación de regresión.\n",
    "\n",
    "3. `activation_OP = tf.nn.sigmoid(add_bias_OP, name=\"activation\")`: Esta línea aplica la función sigmoide al resultado de la suma del sesgo. La función sigmoide transforma los valores a un rango entre 0 y 1, lo que permite interpretar la salida como una probabilidad.\n",
    "\n",
    "4. `return activation_OP`: Finalmente, la función devuelve el resultado de la función sigmoide. Este es el valor de probabilidad predicho por el modelo para la clase positiva.\n",
    "\n",
    "En resumen, esta función toma los datos de entrada, aplica los pesos y el sesgo, y luego pasa el resultado a través de la función sigmoide para obtener una probabilidad. Esta es la esencia de cómo funciona la regresión logística.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"ref3\"></a>\n",
    "\n",
    "<h2>Entrenamiento</h2>\n",
    "\n",
    "El algoritmo de aprendizaje es cómo buscamos el mejor vector de peso (${\\\\\\\\bf w}$). Esta búsqueda es un problema de optimización que busca la hipótesis que optimice una medida de error/coste.\n",
    "\n",
    "<b>¿Qué nos dice que nuestro modelo es malo?</b>\n",
    "\n",
    "El Costo o Pérdida del modelo, entonces lo que queremos es minimizarlo.\n",
    "\n",
    "<h3>Función de coste</h3>\n",
    "\n",
    "Antes de definir nuestra función de costos, debemos definir cuánto tiempo vamos a entrenar y cómo debemos definir la tasa de aprendizaje."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of Epochs in our training\n",
    "numEpochs = 700\n",
    "\n",
    "# Defining our learning rate iterations (decay)\n",
    "learningRate = tf.keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=0.0008,\n",
    "                                          decay_steps=x_train.shape[0],\n",
    "                                          decay_rate= 0.95,\n",
    "                                          staircase=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este fragmento de código está configurando dos aspectos importantes del entrenamiento de tu modelo de regresión logística: el número de épocas y la tasa de aprendizaje.\n",
    "\n",
    "1. `numEpochs = 700`: Aquí estás definiendo el número de épocas para el entrenamiento de tu modelo. Una época es una iteración completa a través de todo el conjunto de datos de entrenamiento. En este caso, estás configurando tu modelo para que se entrene durante 700 épocas.\n",
    "\n",
    "2. `learningRate = tf.keras.optimizers.schedules.ExponentialDecay(...)`: Aquí estás definiendo la tasa de aprendizaje de tu modelo utilizando una decaimiento exponencial. La tasa de aprendizaje controla cuánto se actualizan los parámetros del modelo en cada paso del entrenamiento. Con el decaimiento exponencial, la tasa de aprendizaje disminuirá exponencialmente a medida que avanza el entrenamiento.\n",
    "\n",
    "   - `initial_learning_rate=0.0008`: Esta es la tasa de aprendizaje inicial.\n",
    "   - `decay_steps=trainX.shape[0]`: Este es el número de pasos de entrenamiento que debe tomar antes de que la tasa de aprendizaje comience a decaer.\n",
    "   - `decay_rate=0.95`: Este es el factor de decaimiento. Una tasa de decaimiento de 0.95 disminuirá la tasa de aprendizaje en un 5% en cada `decay_steps`.\n",
    "   - `staircase=True`: Si es `True`, la tasa de aprendizaje decae de manera discreta en lugar de continua, como si bajara por una escalera.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>¿Cuál es la función de costos en nuestro modelo?</b>\n",
    "La función de costo que vamos a utilizar es la función de pérdida de error medio cuadrático.\n",
    "\n",
    "<b>¿Cómo minimizar la función de costes?</b>\n",
    "\n",
    "No podemos usar <b>regresión lineal de mínimos cuadrados</b> \n",
    "aquí, por lo que usaremos descenso de gradiente</a> en su lugar. . Específicamente, utilizaremos el descenso de gradiente por lotes, que calcula el gradiente a partir de todos los puntos de datos del conjunto de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creamos la funcion de perdida usando la error medio cuadratico\n",
    "loss_object = tf.keras.losses.MeanSquaredLogarithmicError()\n",
    "\n",
    "optimizer = tf.keras.optimizers.SGD(learningRate) #Optimizamos la taza de aprendizaje con la funcion de keras\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy metric.\n",
    "def accuracy(y_pred, y_true):\n",
    "# Predicted class is the index of the highest score in prediction vector (i.e. argmax).\n",
    "\n",
    "    correct_prediction = tf.equal(tf.argmax(y_pred, 1), tf.argmax(y_true, 1))\n",
    "\n",
    "    return tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta función `accuracy` calcula la precisión de las predicciones de un modelo de clasificación. Aquí te explico cada parte:\n",
    "\n",
    "1. `correct_prediction = tf.equal(tf.argmax(y_pred, 1), tf.argmax(y_true, 1))`: Esta línea compara las clases predichas con las clases verdaderas. `tf.argmax(y_pred, 1)` devuelve la clase predicha, que es el índice de la mayor probabilidad en cada vector de predicción. `tf.argmax(y_true, 1)` devuelve la clase verdadera, que es el índice del valor 1 en cada vector de etiqueta verdadera (asumiendo que las etiquetas verdaderas están en formato one-hot). `tf.equal` devuelve `True` para las predicciones correctas y `False` para las incorrectas.\n",
    "\n",
    "2. `return tf.reduce_mean(tf.cast(correct_prediction, tf.float32))`: Esta línea calcula la precisión del modelo. Primero, `tf.cast(correct_prediction, tf.float32)` convierte la matriz de booleanos `correct_prediction` en una matriz de flotantes, con `True` convertido en `1.0` y `False` convertido en `0.0`. Luego, `tf.reduce_mean` calcula la media de esta matriz, que es la proporción de predicciones correctas, es decir, la precisión del modelo.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimization process. , Hallamos los gradientes para poder usarlos en el modelo\n",
    "\n",
    "def run_optimization(x, y):\n",
    "    with tf.GradientTape() as g:\n",
    "        pred = logistic_regression(x)\n",
    "        loss = loss_object(pred, y)\n",
    "    gradients = g.gradient(loss, [weights, bias])\n",
    "    optimizer.apply_gradients(zip(gradients, [weights, bias]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La función `run_optimization` es una parte esencial del entrenamiento de un modelo de regresión logística en TensorFlow. Aquí está lo que hace cada línea de código:\n",
    "\n",
    "1. `with tf.GradientTape() as g:`: TensorFlow utiliza un objeto `GradientTape` para registrar las operaciones que se realizan dentro de su contexto. Esto permite calcular automáticamente los gradientes de las variables con respecto a una pérdida.\n",
    "\n",
    "2. `pred = logistic_regression(x)`: Esta línea predice las salidas (o etiquetas) para las entradas dadas `x` utilizando el modelo de regresión logística.\n",
    "\n",
    "3. `loss = loss_object(pred, y)`: Aquí se calcula la pérdida entre las etiquetas predichas `pred` y las etiquetas reales `y` utilizando un objeto de pérdida (por ejemplo, la entropía cruzada para la clasificación).\n",
    "\n",
    "4. `gradients = g.gradient(loss, [weights, bias])`: Esta línea calcula los gradientes de la pérdida con respecto a las variables del modelo (en este caso, los pesos y el sesgo). Estos gradientes se utilizarán para actualizar las variables del modelo.\n",
    "\n",
    "5. `optimizer.apply_gradients(zip(gradients, [weights, bias]))`: Finalmente, esta línea aplica los gradientes calculados a las variables del modelo utilizando un optimizador. Esto actualiza los pesos y el sesgo en la dirección que minimiza la pérdida.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loop de entreno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Cristian\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\utils\\losses_utils.py:325: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "step 0, training accuracy 0.397849, loss 0.134017, change in loss 0.134017\n",
      "step 10, training accuracy 0.397849, loss 0.133457, change in loss 0.00056009\n",
      "step 20, training accuracy 0.397849, loss 0.132906, change in loss 0.000551552\n",
      "step 30, training accuracy 0.397849, loss 0.132363, change in loss 0.000542998\n",
      "step 40, training accuracy 0.397849, loss 0.131828, change in loss 0.00053452\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 50, training accuracy 0.397849, loss 0.131302, change in loss 0.000526026\n",
      "step 60, training accuracy 0.397849, loss 0.130784, change in loss 0.000517607\n",
      "step 70, training accuracy 0.397849, loss 0.130275, change in loss 0.000509217\n",
      "step 80, training accuracy 0.397849, loss 0.129774, change in loss 0.000500873\n",
      "step 90, training accuracy 0.397849, loss 0.129282, change in loss 0.000492617\n",
      "step 100, training accuracy 0.397849, loss 0.128817, change in loss 0.00046514\n",
      "step 110, training accuracy 0.397849, loss 0.128364, change in loss 0.000452921\n",
      "step 120, training accuracy 0.397849, loss 0.127918, change in loss 0.000445589\n",
      "step 130, training accuracy 0.397849, loss 0.12748, change in loss 0.000438362\n",
      "step 140, training accuracy 0.397849, loss 0.127048, change in loss 0.00043121\n",
      "step 150, training accuracy 0.397849, loss 0.126624, change in loss 0.000424087\n",
      "step 160, training accuracy 0.397849, loss 0.126207, change in loss 0.000417024\n",
      "step 170, training accuracy 0.397849, loss 0.125797, change in loss 0.000410095\n",
      "step 180, training accuracy 0.397849, loss 0.125394, change in loss 0.000403196\n",
      "step 190, training accuracy 0.397849, loss 0.125007, change in loss 0.000386536\n",
      "step 200, training accuracy 0.397849, loss 0.124637, change in loss 0.000370488\n",
      "step 210, training accuracy 0.397849, loss 0.124273, change in loss 0.000364453\n",
      "step 220, training accuracy 0.397849, loss 0.123914, change in loss 0.000358537\n",
      "step 230, training accuracy 0.397849, loss 0.123561, change in loss 0.000352673\n",
      "step 240, training accuracy 0.397849, loss 0.123214, change in loss 0.000346862\n",
      "step 250, training accuracy 0.397849, loss 0.122873, change in loss 0.000341155\n",
      "step 260, training accuracy 0.397849, loss 0.122538, change in loss 0.0003355\n",
      "step 270, training accuracy 0.397849, loss 0.122208, change in loss 0.000329934\n",
      "step 280, training accuracy 0.397849, loss 0.121887, change in loss 0.000321209\n",
      "step 290, training accuracy 0.397849, loss 0.121583, change in loss 0.000303246\n",
      "step 300, training accuracy 0.397849, loss 0.121285, change in loss 0.000298411\n",
      "step 310, training accuracy 0.397849, loss 0.120991, change in loss 0.000293657\n",
      "step 320, training accuracy 0.397849, loss 0.120702, change in loss 0.000288963\n",
      "step 330, training accuracy 0.397849, loss 0.120418, change in loss 0.000284329\n",
      "step 340, training accuracy 0.397849, loss 0.120138, change in loss 0.000279777\n",
      "step 350, training accuracy 0.397849, loss 0.119863, change in loss 0.000275262\n",
      "step 360, training accuracy 0.397849, loss 0.119592, change in loss 0.000270844\n",
      "step 370, training accuracy 0.397849, loss 0.119326, change in loss 0.000266485\n",
      "step 380, training accuracy 0.397849, loss 0.119075, change in loss 0.000250481\n",
      "step 390, training accuracy 0.397849, loss 0.11883, change in loss 0.000245318\n",
      "step 400, training accuracy 0.397849, loss 0.118588, change in loss 0.000241555\n",
      "step 410, training accuracy 0.397849, loss 0.118351, change in loss 0.000237837\n",
      "step 420, training accuracy 0.397849, loss 0.118116, change in loss 0.000234194\n",
      "step 430, training accuracy 0.397849, loss 0.117886, change in loss 0.000230581\n",
      "step 440, training accuracy 0.397849, loss 0.117659, change in loss 0.000227042\n",
      "step 450, training accuracy 0.397849, loss 0.117435, change in loss 0.000223547\n",
      "step 460, training accuracy 0.397849, loss 0.117215, change in loss 0.00022009\n",
      "step 470, training accuracy 0.397849, loss 0.117005, change in loss 0.000210263\n",
      "step 480, training accuracy 0.397849, loss 0.116802, change in loss 0.000202879\n",
      "step 490, training accuracy 0.397849, loss 0.116602, change in loss 0.000199907\n",
      "step 500, training accuracy 0.397849, loss 0.116405, change in loss 0.000196986\n",
      "step 510, training accuracy 0.397849, loss 0.116211, change in loss 0.00019414\n",
      "step 520, training accuracy 0.397849, loss 0.11602, change in loss 0.000191271\n",
      "step 530, training accuracy 0.397849, loss 0.115831, change in loss 0.000188485\n",
      "step 540, training accuracy 0.397849, loss 0.115645, change in loss 0.00018575\n",
      "step 550, training accuracy 0.397849, loss 0.115462, change in loss 0.000183038\n",
      "step 560, training accuracy 0.397849, loss 0.115285, change in loss 0.000177674\n",
      "step 570, training accuracy 0.397849, loss 0.115116, change in loss 0.000168987\n",
      "step 580, training accuracy 0.397849, loss 0.114949, change in loss 0.000166625\n",
      "step 590, training accuracy 0.397849, loss 0.114785, change in loss 0.00016433\n",
      "step 600, training accuracy 0.397849, loss 0.114623, change in loss 0.00016208\n",
      "step 610, training accuracy 0.397849, loss 0.114463, change in loss 0.00015983\n",
      "step 620, training accuracy 0.397849, loss 0.114305, change in loss 0.000157654\n",
      "step 630, training accuracy 0.397849, loss 0.11415, change in loss 0.000155479\n",
      "step 640, training accuracy 0.397849, loss 0.113996, change in loss 0.00015337\n",
      "step 650, training accuracy 0.397849, loss 0.113845, change in loss 0.000151254\n",
      "step 660, training accuracy 0.397849, loss 0.113703, change in loss 0.000141792\n",
      "step 670, training accuracy 0.397849, loss 0.113563, change in loss 0.000139944\n",
      "step 680, training accuracy 0.397849, loss 0.113425, change in loss 0.000138134\n",
      "step 690, training accuracy 0.397849, loss 0.113289, change in loss 0.000136361\n",
      "final accuracy on test set: 0.39784947\n"
     ]
    }
   ],
   "source": [
    "# Initialize reporting variables\n",
    "display_step = 10\n",
    "epoch_values = []\n",
    "accuracy_values = []\n",
    "loss_values = []\n",
    "loss = 0\n",
    "diff = 1\n",
    "# Training epochs\n",
    "for i in range(numEpochs):\n",
    "    if i > 1 and diff < .0001:\n",
    "        print(\"change in loss %g; convergence.\"%diff)\n",
    "        break\n",
    "    else:\n",
    "        # Run training step\n",
    "        run_optimization(x_train, y_train)\n",
    "        \n",
    "        # Report occasional stats\n",
    "        if i % display_step == 0:\n",
    "            # Add epoch to epoch_values\n",
    "            epoch_values.append(i)\n",
    "            \n",
    "            pred = logistic_regression(x_train)\n",
    "\n",
    "            newLoss = loss_object(pred, y_train)\n",
    "            # Add loss to live graphing variable\n",
    "            loss_values.append(newLoss)\n",
    "            \n",
    "            # Generate accuracy stats on test data\n",
    "            acc = accuracy(pred, y_train)\n",
    "            accuracy_values.append(acc)\n",
    "            \n",
    "    \n",
    "            # Re-assign values for variables\n",
    "            diff = abs(newLoss - loss)\n",
    "            loss = newLoss\n",
    "\n",
    "            #generate print statements\n",
    "            print(\"step %d, training accuracy %g, loss %g, change in loss %g\"%(i, acc, newLoss, diff))\n",
    "\n",
    "        \n",
    "\n",
    "          \n",
    "\n",
    "# How well do we perform on held-out test data?\n",
    "print(\"final accuracy on test set: %s\" %acc.numpy())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1. **Inicialización de variables**: Al principio, se inicializan varias variables que se utilizarán para rastrear el progreso del entrenamiento. Esto incluye listas para almacenar los valores de las épocas, la precisión y la pérdida, así como variables para la pérdida y la diferencia en la pérdida entre las épocas.\n",
    "\n",
    "2. **Bucle de entrenamiento**: El bucle `for` recorre un número determinado de épocas de entrenamiento. En cada época, se realiza un paso de optimización (es decir, se actualizan los pesos del modelo) utilizando la función `run_optimization`.\n",
    "\n",
    "3. **Convergencia temprana**: Si la diferencia en la pérdida entre las épocas es menor que un cierto umbral (en este caso, 0.0001), el algoritmo concluye que el modelo ha convergido y detiene el entrenamiento.\n",
    "\n",
    "4. **Registro de estadísticas**: En cada época, si la época es un múltiplo de `display_step`, el algoritmo calcula la pérdida y la precisión del modelo en los datos de prueba, y registra estos valores. También calcula la diferencia en la pérdida desde la última época.\n",
    "\n",
    "5. **Impresión de resultados**: Finalmente, una vez que se ha completado el entrenamiento, el algoritmo imprime la precisión final del modelo en el conjunto de prueba.\n",
    "\n",
    "En resumen, este algoritmo está entrenando un modelo de regresión logística, monitoreando su rendimiento a lo largo del tiempo, y deteniendo el entrenamiento una vez que el modelo ha convergido.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1. `for i in range(numEpochs):` Este es el inicio del bucle `for`, que se ejecutará para un número determinado de épocas (iteraciones completas a través del conjunto de datos de entrenamiento). El número de épocas es `numEpochs`.\n",
    "\n",
    "2. `if i > 1 and diff < .0001:` Aquí se comprueba si la diferencia en la pérdida entre las épocas consecutivas es menor que 0.0001, y si se han completado al menos dos épocas. Si ambas condiciones se cumplen, significa que el modelo ha convergido y se detiene el entrenamiento.\n",
    "\n",
    "3. `run_optimization(trainX, trainY)`: Aquí se realiza un paso de optimización, que actualiza los pesos del modelo basándose en los gradientes de la función de pérdida. La función `run_optimization` se aplica al conjunto de entrenamiento.\n",
    "\n",
    "4. `if i % display_step == 0:` Aquí se comprueba si la época actual es un múltiplo de `display_step`. Si es así, se calculan y registran las estadísticas del modelo.\n",
    "\n",
    "5. `epoch_values.append(i)`: Aquí se añade el número de la época actual a la lista `epoch_values`.\n",
    "\n",
    "6. `pred = logistic_regression(testX)`: Aquí se calculan las predicciones del modelo para el conjunto de prueba.\n",
    "\n",
    "7. `newLoss = loss_object(pred, testY)`: Aquí se calcula la pérdida del modelo en el conjunto de prueba.\n",
    "\n",
    "8. `loss_values.append(newLoss)`: Aquí se añade la pérdida calculada a la lista `loss_values`.\n",
    "\n",
    "9. `acc = accuracy(pred, testY)`: Aquí se calcula la precisión del modelo en el conjunto de prueba.\n",
    "\n",
    "10. `accuracy_values.append(acc)`: Aquí se añade la precisión calculada a la lista `accuracy_values`.\n",
    "\n",
    "11. `diff = abs(newLoss - loss)`: Aquí se calcula la diferencia absoluta en la pérdida desde la última época.\n",
    "\n",
    "12. `loss = newLoss`: Aquí se actualiza el valor de la pérdida para la próxima iteración.\n",
    "\n",
    "Espero que esto te ayude a entender mejor lo que está sucediendo en el bucle `for`. Si tienes más preguntas, no dudes en hacerlas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.plot(loss_values)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "1. **Disminución de la pérdida**: La pérdida de tu modelo parece estar disminuyendo a medida que aumenta el número de épocas. Esto es una buena señal y sugiere que tu modelo está aprendiendo de los datos de entrenamiento y mejorando su rendimiento a lo largo del tiempo.\n",
    "\n",
    "2. **Convergencia del modelo**: La curva de pérdida parece estar acercándose a un valor mínimo y estabilizándose, lo que sugiere que tu modelo puede estar convergiendo. Esto significa que las actualizaciones adicionales a los pesos del modelo durante el entrenamiento están resultando en mejoras incrementales más pequeñas en la pérdida del modelo.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regresion lineal\n",
    "\n",
    "**Vamos a crear una regresion lineal simple usando tensorflow**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10,), dtype=float32, numpy=\n",
       "array([  4.,   9.,  16.,  25.,  36.,  49.,  64.,  81., 100., 121.],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Aqui tenemos nuestros datos con lo que se creara la regresion\n",
    "\n",
    "X=tf.constant(range(10),dtype=tf.float32)\n",
    "\n",
    "Y=tf.square(2+X)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'weight:0' shape=() dtype=float32, numpy=-0.9172429> <tf.Variable 'bias:0' shape=() dtype=float32, numpy=0.35401145>\n"
     ]
    }
   ],
   "source": [
    "#Creamos el peso y el sesgo\n",
    "\n",
    "W = tf.Variable(np.random.randn(), name=\"weight\")\n",
    "b = tf.Variable(np.random.randn(), name=\"bias\")\n",
    "\n",
    "print(W,b)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el contexto de los modelos de machine learning y las redes neuronales, los términos \"peso\" y \"sesgo\" se refieren a dos tipos de parámetros que el modelo aprende de los datos durante el entrenamiento.\n",
    "\n",
    "- **Peso (W)**: Los pesos son los coeficientes de las variables de entrada en la función de predicción del modelo. En una red neuronal, los pesos son los factores que se multiplican por las entradas de las neuronas para calcular la salida. Los pesos determinan la importancia relativa de las características de entrada en la predicción del resultado.\n",
    "\n",
    "- **Sesgo (b)**: El sesgo es un término constante que se añade a la función de predicción del modelo para desplazar la salida. En una red neuronal, el sesgo permite que la neurona produzca una salida distinta de cero cuando todas las entradas son cero. El sesgo ayuda a ajustar la salida del modelo a los datos.\n",
    "\n",
    "En una regresión lineal, los pesos y los sesgos corresponden a la pendiente y la intersección de la línea de regresión, respectivamente. En tu código, `W` y `b` se inicializan como variables de TensorFlow vacías, lo que significa que sus valores se establecerán durante el entrenamiento del modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`W` y `b` son los parámetros del modelo en una regresión lineal.\n",
    "\n",
    "- `W` es el peso o coeficiente para la variable independiente. En el contexto de la regresión lineal, `W` representa la pendiente de la línea de regresión. Es decir, `W` es la cantidad en la que la variable dependiente (la que estamos tratando de predecir) cambia por cada cambio de una unidad en la variable independiente.\n",
    "\n",
    "- `b` es el sesgo o la intersección y. Representa el punto en el que la línea de regresión corta el eje y cuando todas las características son cero. En otras palabras, `b` es la predicción que haríamos si todas nuestras variables independientes fueran cero.\n",
    "\n",
    "En una ecuación de regresión lineal simple `y = Wx + b`, `y` es la variable dependiente que estamos tratando de predecir, `x` es la variable independiente que estamos usando para hacer la predicción, `W` es el peso de `x`, y `b` es el sesgo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_Regression(x:float)->float:\n",
    "\n",
    "    return W* x + b\n",
    "\n",
    "def mean_square(y_pred, y_true):\n",
    "    return tf.reduce_mean(tf.square(y_pred - y_true))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MSE\n",
    "El Error Cuadrático Medio es una métrica común para evaluar el rendimiento de un modelo de regresión. Se calcula como el promedio de los cuadrados de las diferencias entre las predicciones del modelo y los valores verdaderos. En otras palabras, para cada punto de datos, calcula la diferencia entre el valor predicho por el modelo y el valor verdadero, eleva al cuadrado esa diferencia, y luego promedia todas estas diferencias al cuadrado.\n",
    "\n",
    "El MSE es útil porque penaliza los errores grandes más que los errores pequeños (debido al cuadrado), y porque es diferenciable, lo que es útil para los algoritmos de optimización como el descenso de gradiente.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Esta función tomaría dos argumentos, las predicciones del modelo (`y_pred`) y los valores verdaderos (`y_true`), y devolvería el error cuadrático medio entre ellos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Definimos la ecuacion de regresion lineal y tambien el learning_Rate junto al number_epochs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_Rate=0.01\n",
    "\n",
    "optimizer = tf.optimizers.SGD(learning_Rate) #Ajusta el modelo y lo optimiza segun el learning_Rate\n",
    "\n",
    "number_epochs=500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usaremos los gradientes de los tensores\n",
    "\n",
    "Los gradientes de los tensores se utilizan en el cálculo de la regresión lineal porque son fundamentales para el proceso de optimización que permite encontrar los mejores parámetros para el modelo.\n",
    "\n",
    "En el contexto de la regresión lineal, los parámetros del modelo son los pesos y los sesgos, y el objetivo es encontrar los valores de estos parámetros que minimizan el error entre las predicciones del modelo y los valores reales. Este error se mide a través de una función de pérdida, como el error cuadrático medio.\n",
    "\n",
    "El gradiente de la función de pérdida con respecto a los parámetros del modelo nos da la dirección en la que debemos cambiar los parámetros para reducir el error. Es decir, nos dice cómo cambiar los pesos y los sesgos para acercarnos a la solución óptima.\n",
    "\n",
    "El Descenso de Gradiente es un algoritmo de optimización utilizado para minimizar una función objetivo, como una función de pérdida en el aprendizaje automático. Funciona de la siguiente manera:\n",
    "\n",
    "1. **Inicialización**: Selecciona un punto de partida aleatorio (valores iniciales para los parámetros del modelo).\n",
    "\n",
    "2. **Cálculo del Gradiente**: Calcula el gradiente de la función de pérdida en el punto actual. El gradiente es un vector que apunta en la dirección de la mayor tasa de aumento de la función, y su magnitud es la tasa de ese aumento.\n",
    "\n",
    "3. **Actualización del Parámetro**: Actualiza los parámetros moviéndote una pequeña cantidad en la dirección del gradiente negativo (la dirección de descenso más empinada). La cantidad que te mueves se determina por el \"tasa de aprendizaje\", un hiperparámetro que debes elegir.\n",
    "\n",
    "4. **Iteración**: Repite los pasos 2 y 3 hasta que el gradiente sea muy cercano a cero (lo que indica que has llegado a un mínimo de la función) o hasta que hayas realizado un número predefinido de iteraciones.\n",
    "\n",
    "El objetivo de este proceso es encontrar los parámetros del modelo que minimizan la función de pérdida. En el contexto del aprendizaje automático, esto significa encontrar los parámetros que hacen que tu modelo se ajuste lo mejor posible a los datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(number_epochs):\n",
    "\n",
    "    #Usando la funcion tape hallamos la predicciones de cada epoca usando la formula de regresion\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_predict=linear_Regression(X)\n",
    "        loss=mean_square(y_predict,Y)\n",
    "\n",
    "    #Calculamos los gradientes\n",
    "    gradients=tape.gradient(loss,[W,b])\n",
    "\n",
    "    optimizer.apply_gradients(zip(gradients, [W, b]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[11.962177]]\n"
     ]
    }
   ],
   "source": [
    "nuevo_x = tf.constant([[1.5]], dtype=tf.float32)\n",
    "\n",
    "prediccion_y = linear_Regression(nuevo_x)\n",
    "\n",
    "print(prediccion_y.numpy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
